
# AI-Generated Text Detection System
[ä¸­æ–‡ç‰ˆ](README.md)
## 1. Project Background

This project is an implementation of a solution for NLPCC 2025 Shared Task 1, "Detecting Text Generated by Large Language Models." The task aims to differentiate between Chinese text generated by large language models and text written by humans, providing technical support to address the challenges posed by AI-generated content.

**Task Goal**: Build an efficient and robust AI-generated content detection system capable of accurately identifying AI-generated Chinese text in various scenarios (especially out-of-distribution data).

## 2. Solution Overview

This project employs the Binoculars detection method, which detects whether text is AI-generated by comparing the predictive distribution differences between an Observer Model and a Performer Model. Specifically:

1.  **Observer Model**: Uses a foundational language model (e.g., Qwen2.5-7B) to compute the baseline perplexity of the text.
2.  **Performer Model**: Uses an instruction-tuned language model (e.g., Qwen2.5-7B-Instruct) to compute the execution perplexity of the text.
3.  **Binoculars Score**: Calculates the ratio of the performer perplexity to the cross-perplexity as a detection feature.

**The Binoculars method is inspired by:**
[Hans et al. (2024)](https://arxiv.org/pdf/2401.12070)

### 2.1 Binoculars Method Formula

The core formula of the Binoculars method is as follows:

$$
B(x) = \frac{\mathrm{PPL}_q(x)}{H(p, q, x)}
$$

Where:

*   $B(x)$ is the Binoculars score of text $x$.
*   $\mathrm{PPL}_q(x)$ is the perplexity of the performer model $q$ for text $x$.
*   $H(p, q, x)$ is the cross-perplexity between the observer model $p$ and the performer model $q$ for text $x$.

Perplexity Calculation Formula:

$$
\mathrm{PPL}(x) = \exp\left(-\frac{1}{|x|} \sum \log P(x_i|x_{<i})\right)
$$

Cross-Perplexity Calculation Formula:

$$
H(p, q, x) = \exp\left(-\frac{1}{|x|} \sum \sum p(j|x_{<i}) \log q(j|x_{<i})\right)
$$

For AI-generated text, the $B(x)$ value is typically small; for human-written text, the $B(x)$ value is typically large. Based on this observation, we use an optimized formula and the optimal threshold for classification:

$$
\text{prediction} =
\begin{cases}
1\ (\text{AI Generated}), & \text{if } \frac{\mathrm{PPL}_q(x)}{\log_x - H(p, q, x)} < T \\
0\ (\text{Human Written}), & \text{if } \frac{\mathrm{PPL}_q(x)}{\log_x - H(p, q, x)} \geq T
\end{cases}
$$

The optimal parameters $\log_x$ and threshold $T$ are determined by optimizing the F1 score on the validation set.

*   log\_x = 7.4146
*   T = 0.4118
*   Validation Set F1 Score = 0.8980 ðŸŽ‰ðŸŽ‰

To further optimize the classification performance, we apply a more complex transformation to the Binoculars score:

$$
B_{\text{modified}}(x) = \frac{\mathrm{PPL}_q(x)}{\log_x - H(p, q, x)}
$$

Where `log_x` is an adjustable parameter used to scale the cross-perplexity, making the Binoculars score more robust to noise. By adjusting the value of `log_x`, we can optimize the classification performance. The project uses the `find_XT.py` script to automatically find the best `log_x` and classification threshold `T`. To verify the effectiveness of the `log_x` parameter, we conducted an ablation study, searching only for the threshold `T` while keeping the original Binoculars score formula unchanged. The experimental results show that introducing and optimizing the `log_x` parameter significantly improves the performance of the AI-generated text detection system. On the validation set, the F1 score of the original method (searching for `log_x` and `T`) was 0.8980, while the F1 score of the method searching only for `T` was 0.8483.

## 3. Key Tools and Scripts

#### 3.1 features_extract.py

This file is the core component of the project, implementing the `BinocularsComputer` class for calculating the Binoculars score of the text. Key features include:

*   **Model Loading and Management**: Loads the observer and performer models on demand and implements memory optimization. **Note: During the first run, this process will automatically download the Qwen2.5-7B and Qwen2.5-7B-Instruct models using the `modelscope` library, which is expected to occupy approximately 30GB of disk space.**
*   **Text Processing**: Converts the input text into tokens and performs batch processing.
*   **Score Calculation**: Calculates perplexity, cross-perplexity, and the final Binoculars score.
*   **Data Batching**: Supports batch processing to accelerate the calculation process.
*   **Result Saving**: Saves the calculation results to the specified directory.

In addition, the file also implements the `BinocularsPredictor` class for performing classification predictions based on pre-calculated Binoculars scores:

*   Automatically calibrates the optimal classification threshold.
*   Performs predictions and evaluates performance.
*   Generates the final submission results.

#### 3.2 find_XT.py

This file is used to find the optimal parameters log\_x and threshold T. It implements a parameter search algorithm to improve classification performance by optimizing the following improved Binoculars formula:

$$
B_{\text{modified}}(x) = \frac{\mathrm{PPL}_q(x)}{\log_x - H(p, q, x)}
$$

The script's main functions include:

*   Loading pre-calculated Binoculars scores from the validation set.
*   Searching for the optimal log\_x value within a given range.
*   For each log\_x value, finding the optimal classification threshold T.
*   Selecting the best parameter combination based on the F1 score.
*   Saving the best parameters to a JSON file.

The running result of this script:

```json
{
  "best_log_x": 7.414581809045226,
  "best_threshold": 0.41181129816517914,
  "best_f1": 0.9218297625940938
}
```

These parameters are then used in the final prediction model to achieve optimal classification performance.

#### 3.3 evaltrain.py

This file is used to evaluate the model performance on the training and development sets and to conduct detailed error analysis. Key functions include:

*   Using pre-calculated optimal parameters (log\_x and threshold T) to perform classification predictions.
*   Calculating and displaying overall classification metrics (accuracy, precision, recall, F1 score).
*   Generating confusion matrices and saving them as visual images.
*   Analyzing detection performance by grouping by model type to discover which AI models are easier to detect.
*   Analyzing by data source to evaluate the detection effect in different text domains.
*   Generating detailed model performance comparison charts (saved as PNG format).

The script outputs various visualization results, including:

*   Confusion matrices (`confusion_matrix_*.png`).
*   Comparison charts of detection accuracy for different models (`model_detection_accuracy_*.png`).
*   Comparison charts of detection accuracy for different data sources (`source_detection_accuracy_*.png`).

These analysis results are valuable for understanding the strengths and limitations of the model and for guiding further optimization.

#### 3.4 prediction.py

This file is used to predict the test set using the trained model and generate the final submission file. Key functions include:

*   Loading the test set data and pre-calculated Binoculars scores.
*   Using pre-set optimal parameters (`log_x = 7.4146` and threshold `T = 0.4118`) to perform classification predictions.
*   Applying the transformation formula to the Binoculars scores.
*   Handling edge cases where the denominator is close to zero, by defaulting the prediction for that sample to human-written to avoid `NaN` values.
*   Formatting the prediction results into a JSON format containing the `id`, `text`, and `label` fields.
*   Saving the prediction results as the submission file `submission.json`.

## 4. Usage Workflow

The complete usage workflow of the project is as follows:

1.  **Install Dependencies**:

    ```bash
    pip install -r requirements.txt
    ```

2.  **Preprocess the dataset**: Use `convert.py` to clean the data in the text of the dataset that contains `\n` line breaks, because it is observed that the AI-generated text contains \n, but the AI text in the test set does not have line breaks, and I am worried that the model will be affected by this feature and affect performance. And convert the format to `jsonl` to facilitate numerical calculation using the Qwen model.

    ```bash
    python src/convert.py --input data/train.json --output data/train.jsonl
    python src/convert.py --input data/dev.json --output data/dev.jsonl
    python src/convert.py --input data/test.json --output data/test.jsonl
    ```

3.  **Feature Extraction**: Use `features_extract.py` to calculate the Binoculars scores on the training, development, and test sets:

    ```bash
    python src/features_extract.py --train_file data/train.jsonl --dev_file data/dev.jsonl --test_file data/test.jsonl --output_dir features
    ```

4.  **Parameter Optimization**: Use `find_XT.py` to find the best log\_x and threshold T parameters on the development set:

    ```bash
    python src/find_XT.py --dev_file features/dev_scores.json --output_params_file best_binoculars_params_optimized_x.json
    ```

5.  **Model Evaluation**: Use `evaltrain.py` to evaluate the model performance on the training and development sets:

    ```bash
    python src/evaltrain.py --train_scores features/train_scores.json --dev_scores features/dev_scores.json --train_original data/train.json --dev_original data/dev.json
    ```

6.  **Generate Prediction Results**: Use the optimized parameters to predict the test set:

    ```bash
    python src/prediction.py --test_file features/test_scores.json --submission_file submission.json
    ```

    *   Use pre-calculated optimal parameters (log\_x and threshold T) to predict the test set data.
    *   Handle edge cases (such as denominators close to zero).
    *   Format the prediction results into the officially required JSON format.
    *   Save the submission file.

## 5. Experimental Results

Using the optimal parameters log\_x = 7.4146 and threshold T = 0.4118 for evaluation, the following performance was achieved:

5.1 **Development Set Results**:

*   Accuracy: 0.9036
*   Precision: 0.9022
*   Recall: 0.8946
*   Macro F1 Score: 0.8980

5.2 **Training Set Results**:

*   Accuracy: 0.8434
*   Precision: 0.9076
*   Recall: 0.8809
*   Macro F1 Score: 0.7971

5.3 **Evaluating Classification Accuracy by Different Models and Sources on `train.json`**:

*   **Detection Performance of Different Models**:

    | Model   | Sample Size | Correctly Identified | Detection Accuracy | Misidentified as Human Text |
    | ------- | ----------- | -------------------- | ------------------ | ------------------------- |
    | glm     | 8063        | 7842                 | 0.9726             | 221                       |
    | qwen    | 8209        | 7983                 | 0.9725             | 226                       |
    | gpt4o   | 8028        | 5581                 | 0.6952             | 2447                      |

*   **Detection Accuracy by Different Sources**:

    | Data Source | Sample Size | Accuracy |
    | ----------- | ----------- | -------- |
    | csl         | 10800       | 0.9036   |
    | cnewsum     | 10800       | 0.8210   |
    | asap        | 10800       | 0.8056   |

## Hardware and Software Environment

*   **GPU**: A100-SXM4-80GB (80GB) \* 1
*   **CPU**: 15 vCPU Intel(R) Xeon(R) Platinum 8358P CPU @ 2.60GHz
*   **Memory**: 120GB
*   **CUDA**: 11.8
*   **Python**: 3.10 (Ubuntu 22.04)
*   **PyTorch**: 2.1.2
*   **Dependencies**: See `requirements.txt` for details.

## Result Analysis

1.  **Model Analysis**: Based on the training set results, we found significant differences in detection capabilities for different models:

    *   Text generated by the glm and qwen models has a very high detection rate, reaching approximately 97%.
    *   Text generated by the gpt4o model has a significantly lower detection rate, only about 70%.
    *   This indicates that text generated by gpt4o is closer to human writing characteristics and is more difficult for detection algorithms to identify.
    * The model uses Qwen2.5-7B (Bai et al., 2024) pre-trained model and Qwen2.5-7B-Instruct (Qwen Team, 2024) instruction fine-tuned model as observers and performers respectively, and the results show that the model has the best resolution effect for glm and qwen models that also belong to Chinese models.

2.  **Domain Analysis**: There are also differences in detection performance across different text domains:

    *   The academic writing (csl) domain has the highest detection accuracy, reaching over 90%.
    *   The news writing (cnewsum) and social media commentary (asap) domains have lower detection accuracy.
    *   This may be because academic texts have stricter formats and expression norms, while social media content is more free and varied in style.

## Citations and Acknowledgments

*   The Binoculars method is inspired by:

    Hans, A., Schwarzschild, A., Cherepanova, V., Kazemi, H., Saha, A., Goldblum, M., ... & Goldstein, T. (2024, July). Spotting LLMs With Binoculars: Zero-Shot Detection of Machine-Generated Text. In *International Conference on Machine Learning* (pp. 17519-17537). PMLR. [Paper Link](https://arxiv.org/pdf/2401.12070)

*   Uses the Qwen2.5-7B and Qwen2.5-7B-Instruct models (Qwen Team, 2024; Bai et al., 2024):

    *   Qwen Team. (2024, September). Qwen2.5: A Party of Foundation Models. [Project Page](https://qwenlm.github.io/blog/qwen2.5/)

    *   Bai, A. Y., Yang, B., Hui, B., Zheng, B., Yu, B., Zhou, C., ... & Fan, Z. (2024). Qwen2 Technical Report. *arXiv preprint arXiv:2407.10671*. [Paper Link](https://arxiv.org/pdf/2412.15115)
* Wu, J., Yang, S., Zhan, R., Yuan, Y., Chao, L. S., & Wong, D. F. (2025). A survey on LLM-generated text detection: Necessity, methods, and future directions. Computational Linguistics, 1-66.[Paper Link](https://direct.mit.edu/coli/article/51/1/275/127462/A-Survey-on-LLM-Generated-Text-Detection-Necessity)
* Wu, J., Zhan, R., Wong, D. F., Yang, S., Yang, X., Yuan, Y., & Chao, L. S. (2024). DetectRL: Benchmarking LLM-Generated Text Detection in Real-World Scenarios. In The Thirty-eighth Conference on Neural Information Processing Systems Datasets and Benchmarks Track.[Paper Link](https://arxiv.org/pdf/2410.23746)

Special thanks to NLP2CT Lab, University of Macau for organizing this shared task, and to Derek, Fai Wong, Junchao Wu, Runzhe Zhan, Yulin Yuan for their support.
